{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mini-project: Speaker Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speaker recognition is an important tasks in many applications. The problem of speaker recognition aims at determining the speaker identity in a provided recording and matches it to a pre-recorded database or a pre-extracted speaker-related features. It can be applied in places such as secured biometric verification applications (e.g. banks) and command detection systems (e.g. smart home devices).\n",
    "\n",
    "There are two types of tasks in the general problem of speaker recognition - **speaker identification** and **speaker verification**. Speaker identification maps a given utterance to a speaker feature, and matches it to a set of stored speaker features to identify which speaker is speaking. Speaker verification extracts the same speaker feature, but generates a *yes/no* output presenting whether the speaker is the target speaker we want to hear. We can reformulate the speaker identification problem to a one-to-many mapping problem or a multi-class classification problem (as what we did for the acoustic event detection problem), and the speaker verifcation problem to a one-to-one mapping problem or a binary classification problem.\n",
    "\n",
    "Moreover, there are two types of problem settings in speaker recognition - **text-dependent** and **text-independent** - depending on whether we want to put any constraints on the contents. Text-dependent speaker recognition systems are designed for certain contents, such as waken words and key words in smart home devices (\"Hi Siri/Cortana/Google\"), while text-independent speaker recognition systems do not make any assumptions on the content. Intuitively text-independent systems are harder to work but are more general, but if you want higher recognition accuracy in certain environments, text-dependent systems are often more suitable.\n",
    "\n",
    "In this mini-project we will implement a simplified text-independent speaker recognition system. The problem of speaker recognition is dominated by the advances of neural networks in recent years (like other tasks), so our implementation will still be on the network architectures and the training objectives. We call this the *end-to-end neural speaker recognition system*. If you are interested in the conventional methods for speaker recognition, I'll put some links at the end of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pipeline Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A standard text-independent speaker recognition pipeline contains an **embedding extraction module** and an **embedding scoring module**. The embedding extraction module takes an utterance as input and generates an utterance-level embedding (speaker feature), and the embedding scoring module estimates a score (which is typically a probability or a similarity measure) by comparing the embedding from a given utterance (which is the query utterance) to the set of pre-extracted embeddings for all available speakers (a dictionary or a speaker embedding database). To calculate the speaker embedding database, the system needs a set of **enrollment** utterances from all of the speakers in order to extract the target embeddings, and the enrollments are used as the training data. Well-known systems such as [***d-vector***](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/41939.pdf) and [***x-vector***](https://danielpovey.com/files/2018_icassp_xvectors.pdf) all follow this pipeline.\n",
    "![](https://www.mathworks.com/matlabcentral/mlc-downloads/downloads/28aafb36-9ce9-498d-8c52-be5983e1073f/dec0d528-18a8-435b-95fc-20d2a5da0854/images/screenshot.jpg)\n",
    "\n",
    "During the training of the end-to-end neural speaker recognition system, we have the following example procedure:\n",
    "1. For each of the utterance in the enrollment data, perform a multi-class classification on the target speaker labels (as what we did for the acoustic event detection problem). The only difference is that the output from the second-last layer needs to be normalized.\n",
    "2. Extract the normalized output from the second-last layer in the system (the input to the final output classification layer) as the speaker embedding feature.\n",
    "3. Gather all speaker embeddings from all the enrollment utterances from a certain speaker and use them (or certain statistics of them) as the single speaker embedding for this speaker.\n",
    "\n",
    "During the scoring of a given query utterance, we have the following example procedure:\n",
    "1. Extract the speaker embedding from the query utterance.\n",
    "2. For speaker identification task, compare it with all the pre-extracted speaker embeddings in the training set via certain distance measures (e.g. cosine similarity), determine the speaker identity of the query utterance by certain pre-defined similarity threshold.\n",
    "3. For speaker verification task, only compare it with the target speaker's pre-extracted embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this mini-project we will use a dataset sampled from Librispeech. There are 50 speakers in our data, and each speaker has 10 training utterances, 3 validation utterances and 1 test utterance. All of them are 6 second long. Similar to the previous homework and the acoustic event detection tutorial, let's prepare the data and generate the magnitude spectrogram features. The wave files are saved in the \"SV_data\" folder.\n",
    "\n",
    "Note that here the speaker label for the utterances are denoted by their directory names. Take a look at the data directory arrangements before you start loading and manipulating the wave files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "import time\n",
    "import h5py\n",
    "import soundfile as sf\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of .wav files: 500\n",
      "Example path: SV_data/train/spk2/utt8.wav spk2\n"
     ]
    }
   ],
   "source": [
    "# TODO: data preparation\n",
    "\n",
    "dir_path = 'SV_data'  # directory path\n",
    "import h5py\n",
    "# walk through the directory, find the files with .wav extension\n",
    "tr_wav_files = []\n",
    "tr_label=[]\n",
    "dir_aud=dir_path+'/train'\n",
    "for (dirpath, dirnames, filenames) in os.walk(dir_aud):\n",
    "    for file in filenames:\n",
    "        if '.wav' in file:\n",
    "            tr_wav_files.append(dirpath+'/'+file)\n",
    "            tr_label.append(dirpath[14:])\n",
    "val_wav_files = []\n",
    "val_label=[]\n",
    "dir_aud=dir_path+'/validate'\n",
    "for (dirpath, dirnames, filenames) in os.walk(dir_aud):\n",
    "    for file in filenames:\n",
    "        if '.wav' in file:\n",
    "            val_wav_files.append(dirpath+'/'+file)\n",
    "            val_label.append(dirpath[17:])\n",
    "            \n",
    "ts_wav_files = []\n",
    "ts_label=[]\n",
    "dir_aud=dir_path+'/test'\n",
    "for (dirpath, dirnames, filenames) in os.walk(dir_aud):\n",
    "    for file in filenames:\n",
    "        if '.wav' in file:\n",
    "            ts_wav_files.append(dirpath+'/'+file)\n",
    "            ts_label.append(dirpath[13:])\n",
    "            \n",
    "        \n",
    "num_data = len(tr_wav_files)\n",
    "print('Number of .wav files: {:2d}'.format(num_data))\n",
    "print('Example path: ' + tr_wav_files[0], tr_label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_name = 'tr_set2.hdf5'\n",
    "val_name = 'val_set2.hdf5'\n",
    "test_name = 'test_set2.hdf5'\n",
    "\n",
    "tr_dataset = h5py.File(tr_name, 'a')\n",
    "val_dataset = h5py.File(val_name, 'a')\n",
    "test_dataset = h5py.File(test_name, 'a')\n",
    "\n",
    "# maximum length of waveforms \n",
    "sr = 16000  # sample rate\n",
    "length_wave_max = 6 * sr\n",
    "\n",
    "# STFT window and hop size\n",
    "n_fft = 512\n",
    "n_hop = n_fft // 4\n",
    "tst_flag=0\n",
    "trn_flag=0\n",
    "val_flag=0\n",
    "all_spk=['spk'+str(idd) for idd in range(1,51)]\n",
    "\n",
    "for i in range(len(tr_wav_files)):\n",
    "    \n",
    "    # first 200 files for training, next 50 for validation, and last 69 for testing\n",
    "    \n",
    "    # load the wavefiles\n",
    "    y, _ = librosa.load(tr_wav_files[i], sr=sr)  # the default sample rate for them is 16kHz, but you can also change that\n",
    "    \n",
    "    # truncate or zero-pad the signal\n",
    "    y = y[:length_wave_max]\n",
    "    if len(y) < length_wave_max:\n",
    "        y = np.concatenate([y, np.zeros(length_wave_max-len(y))])\n",
    "    \n",
    "    # calculate log-power spectrogram in decibel scale\n",
    "    spec = librosa.stft(y, n_fft=n_fft, hop_length=n_hop)\n",
    "    # the 1e-8 here is added for numerical stability, in case that log10(0) happens\n",
    "    log_power_spec = 10*np.log10(np.abs(spec)**2 + 1e-8)  # shape: (n_fft/2+1, T)\n",
    "    num_frame = log_power_spec.shape[1]  # this should be the same for all the utterances, since we map them into same length\n",
    "    \n",
    "    # save them into the hdf5 file\n",
    "    \n",
    "    if i == 0:\n",
    "        # create sub-datasets\n",
    "        tr_dataset.create_dataset('waveform', shape=(len(tr_wav_files), length_wave_max), dtype=np.float32)\n",
    "        tr_dataset.create_dataset('spec', shape=(len(tr_wav_files), n_fft//2+1, num_frame), dtype=np.float32)\n",
    "        tr_dataset.create_dataset('label', shape=(len(tr_wav_files),), dtype=np.float32)\n",
    "\n",
    "    tr_dataset['waveform'][i] = y\n",
    "    tr_dataset['spec'][i] = log_power_spec\n",
    "    tr_dataset['label'][i] =  all_spk.index(tr_label[i])\n",
    "    \n",
    "    \n",
    "    \n",
    "for i in range(len(val_wav_files)):\n",
    "    \n",
    "    # first 200 files for training, next 50 for validation, and last 69 for testing\n",
    "    \n",
    "    # load the wavefiles\n",
    "    y, _ = librosa.load(val_wav_files[i], sr=sr)  # the default sample rate for them is 16kHz, but you can also change that\n",
    "    \n",
    "    # truncate or zero-pad the signal\n",
    "    y = y[:length_wave_max]\n",
    "    if len(y) < length_wave_max:\n",
    "        y = np.concatenate([y, np.zeros(length_wave_max-len(y))])\n",
    "    \n",
    "    # calculate log-power spectrogram in decibel scale\n",
    "    spec = librosa.stft(y, n_fft=n_fft, hop_length=n_hop)\n",
    "    # the 1e-8 here is added for numerical stability, in case that log10(0) happens\n",
    "    log_power_spec = 10*np.log10(np.abs(spec)**2 + 1e-8)  # shape: (n_fft/2+1, T)\n",
    "    num_frame = log_power_spec.shape[1]  # this should be the same for all the utterances, since we map them into same length\n",
    "    \n",
    "    # save them into the hdf5 file\n",
    "    \n",
    "    if i == 0:\n",
    "        # create sub-datasets\n",
    "        val_dataset.create_dataset('waveform', shape=(len(val_wav_files), length_wave_max), dtype=np.float32)\n",
    "        val_dataset.create_dataset('spec', shape=(len(val_wav_files), n_fft//2+1, num_frame), dtype=np.float32)\n",
    "        val_dataset.create_dataset('label', shape=(len(val_wav_files),), dtype=np.float32)\n",
    "\n",
    "    val_dataset['waveform'][i] = y\n",
    "    val_dataset['spec'][i] = log_power_spec\n",
    "    val_dataset['label'][i] =  all_spk.index(val_label[i])\n",
    "    \n",
    "    \n",
    "for i in range(len(ts_wav_files)):\n",
    "    \n",
    "    # first 200 files for training, next 50 for validation, and last 69 for testing\n",
    "    \n",
    "    # load the wavefiles\n",
    "    y, _ = librosa.load(ts_wav_files[i], sr=sr)  # the default sample rate for them is 16kHz, but you can also change that\n",
    "    \n",
    "    # truncate or zero-pad the signal\n",
    "    y = y[:length_wave_max]\n",
    "    if len(y) < length_wave_max:\n",
    "        y = np.concatenate([y, np.zeros(length_wave_max-len(y))])\n",
    "    \n",
    "    # calculate log-power spectrogram in decibel scale\n",
    "    spec = librosa.stft(y, n_fft=n_fft, hop_length=n_hop)\n",
    "    # the 1e-8 here is added for numerical stability, in case that log10(0) happens\n",
    "    log_power_spec = 10*np.log10(np.abs(spec)**2 + 1e-8)  # shape: (n_fft/2+1, T)\n",
    "    num_frame = log_power_spec.shape[1]  # this should be the same for all the utterances, since we map them into same length\n",
    "    \n",
    "    # save them into the hdf5 file\n",
    "    \n",
    "    if i == 0:\n",
    "        # create sub-datasets\n",
    "        test_dataset.create_dataset('waveform', shape=(len(ts_wav_files), length_wave_max), dtype=np.float32)\n",
    "        test_dataset.create_dataset('spec', shape=(len(ts_wav_files), n_fft//2+1, num_frame), dtype=np.float32)\n",
    "        test_dataset.create_dataset('label', shape=(len(ts_wav_files),), dtype=np.float32)\n",
    "\n",
    "    test_dataset['waveform'][i] = y\n",
    "    test_dataset['spec'][i] = log_power_spec\n",
    "    test_dataset['label'][i] =  all_spk.index(ts_label[i])\n",
    "\n",
    "    \n",
    "tr_dataset.close()\n",
    "val_dataset.close()\n",
    "test_dataset.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the training set\n",
    "tr_dataset = h5py.File(tr_name, 'a')\n",
    "tr_spec = tr_dataset['spec']  # shape: (num_data, n_fft/2+1, T)\n",
    "tr_spec = np.transpose(tr_spec[:], (0, 2, 1)).reshape(-1, n_fft//2+1)  # shape: (num_data*T, n_fft/2+1)\n",
    "tr_mean = np.mean(tr_spec, axis=0)  # shape: (n_fft/2+1)\n",
    "tr_var = np.var(tr_spec, axis=0)  # shape: (n_fft/2+1)\n",
    "tr_std = np.sqrt(tr_var + 1e-8)  # again for numerical stability\n",
    "\n",
    "# apply normalization to all the datasets\n",
    "\n",
    "val_dataset = h5py.File(val_name, 'a')\n",
    "test_dataset = h5py.File(test_name, 'a')\n",
    "\n",
    "tr_dataset['spec'][:] = (tr_dataset['spec'][:] - tr_mean.reshape(1,-1,1)) / tr_std.reshape(1,-1,1)\n",
    "val_dataset['spec'][:] = (val_dataset['spec'][:] - tr_mean.reshape(1,-1,1)) / tr_std.reshape(1,-1,1)\n",
    "test_dataset['spec'][:] = (test_dataset['spec'][:] - tr_mean.reshape(1,-1,1)) / tr_std.reshape(1,-1,1)\n",
    "\n",
    "tr_dataset.close()\n",
    "val_dataset.close()\n",
    "test_dataset.close()\n",
    "\n",
    "# save the mean and std information in files\n",
    "np.save('training_mean', tr_mean)\n",
    "np.save('training_std', tr_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "# a class to load the saved h5py dataset\n",
    "class dataset_pipeline(Dataset):\n",
    "    def __init__(self, path):\n",
    "        super(dataset_pipeline, self).__init__()\n",
    "\n",
    "        self.h5pyLoader = h5py.File(path, 'r')\n",
    "        \n",
    "        self.spec = self.h5pyLoader['spec']\n",
    "        self.label=self.h5pyLoader['label']\n",
    "        \n",
    "        self._len = self.spec.shape[0]  # number of utterances\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        spec_item = torch.from_numpy(self.spec[index].astype(np.float32))\n",
    "        label_item=self.label[index].astype(np.float32)\n",
    "        #label_item = torch.from_numpy([self.label[index].astype(np.float32)])\n",
    "        return spec_item, label_item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self._len\n",
    "    \n",
    "# define data loaders\n",
    "train_loader = DataLoader(dataset_pipeline('tr_set2.hdf5'), \n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=True,  # this ensures that the sequential order of the training samples will be shuffled for different training epochs\n",
    "                         )\n",
    "\n",
    "validation_loader = DataLoader(dataset_pipeline('val_set2.hdf5'), \n",
    "                               batch_size=batch_size, \n",
    "                               shuffle=False,  # typically we fix the sequential order of the validation samples\n",
    "                              )\n",
    "test_loader = DataLoader(dataset_pipeline('test_set2.hdf5'), \n",
    "                               batch_size=1, \n",
    "                               shuffle=False,  # typically we fix the sequential order of the validation samples\n",
    "                              )\n",
    "dataset_len = len(train_loader)\n",
    "log_step = dataset_len // 4\n",
    "print(dataset_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the same AlexNet architecture as the one used in the acoustic event detection task. The only difference, as we mentioned above, is that the output at the second-last layer has to be normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 50]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=50):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=11, stride=4, padding=2),  # number of input channel is 1 (for image it is 3) \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(16, 32, kernel_size=5, padding=2),  # we make the number of hidden channels smaller in these layers\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((3, 3))  # perform adaptive mean pooling on any size of the input to match the provided size\n",
    "        self.classifier = nn.Sequential(\n",
    "            # nn.Dropout()  no Droupout layers here\n",
    "            nn.Linear(64 * 3 * 3, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.Dropout()  no Droupout layers here\n",
    "            nn.Linear(256, 256),\n",
    "            nn.Tanh() # use Tanh instead of ReLU since the output here will be used for the speaker embeddings\n",
    "        )\n",
    "        self.output = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)  # the dimension after adaptive average pooling is (batch, 64, 3, 3)\n",
    "        x = torch.flatten(x, 1)  # average\n",
    "        x = self.classifier(x)  \n",
    "        # normalize before the last layer\n",
    "        embedding = x / (x.pow(2).sum(1) + 1e-6).sqrt().unsqueeze(1)\n",
    "        # output layer\n",
    "        x = self.output(embedding)\n",
    "        return x, embedding\n",
    "    \n",
    "# test it with a sample input\n",
    "model = AlexNet()\n",
    "sample_input = torch.randn(2, 1, 257, 626)  # (batch_size, num_channel, width, height)\n",
    "sample_output, sample_embedding = model(sample_input)\n",
    "print(sample_output.shape, sample_embedding.shape)  # (batch_size, num_classes), (batch_size, embedding_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training and Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, the training of the model is identical as what we have done for acoustic event detection. You should still report the overall accuracy with your best model on the 50 **test** utterances. You need to achieve at least 50% overall accuracy on the **validation** set to get the full mark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: train the model on the dataset above\n",
    "\n",
    "def Accuracy(prediction, label):\n",
    "    \"\"\"\n",
    "    BCE between the predicted probabilities and the target labels.\n",
    "    args:\n",
    "        prediction: shape (num_frame,), torch tensor\n",
    "        label: shape (num_frame,), torch tensor\n",
    "    output:\n",
    "        acc_value: float, a single value, a percentage value between 0 and 100 (corresponds to 0% and 100% accuracy)\n",
    "    \"\"\"\n",
    "    batch_size = prediction.shape[0]\n",
    "\n",
    "\n",
    "    acc_value= 1.0*(prediction==label)\n",
    "    acc_value=acc_value.mean()*100\n",
    "            \n",
    "    return acc_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossF = nn.CrossEntropyLoss()\n",
    "def train(model, epoch, versatile=True):\n",
    "    start_time = time.time()\n",
    "    model = model.train()  # set the model to training mode. Always do this before you start training!\n",
    "    train_loss = 0.\n",
    "    \n",
    "    # load batch data\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        batch_Sp = data[0].unsqueeze(1)\n",
    "        batch_label= data[1]\n",
    "        \n",
    "        # clean up the gradients in the optimizer\n",
    "        # this should be called for each batch\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output= model(batch_Sp)\n",
    "        #output=torch.argmax(output,dim=1)\n",
    "        \n",
    "        #print('o', output[0])\n",
    "        # MSE as objective\n",
    "        #print('l',batch_label)\n",
    "        loss = lossF(output[0], batch_label.long())\n",
    "        \n",
    "        # automatically calculate the backward pass\n",
    "        loss.sum().backward()\n",
    "        # perform the actual backpropagation\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.sum().data.item()\n",
    "        \n",
    "        # OPTIONAL: you can print the training progress \n",
    "        if versatile:\n",
    "            if (batch_idx+1) % log_step == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                print('| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | BCE {:5.4f} |'.format(\n",
    "                    epoch, batch_idx+1, len(train_loader),\n",
    "                    elapsed * 1000 / (batch_idx+1), \n",
    "                    train_loss / (batch_idx+1)\n",
    "                    ))\n",
    "    \n",
    "    train_loss /= (batch_idx+1)\n",
    "    print('-' * 99)\n",
    "    print('    | end of training epoch {:3d} | time: {:5.2f}s | BCE {:5.4f} |'.format(\n",
    "            epoch, (time.time() - start_time), train_loss))\n",
    "    \n",
    "    return train_loss\n",
    "        \n",
    "def validate(model, epoch):\n",
    "    start_time = time.time()\n",
    "    model = model.eval()  # set the model to evaluation mode. Always do this during validation or test phase!\n",
    "    validation_loss = 0.\n",
    "    \n",
    "    # load batch data\n",
    "    for batch_idx, data in enumerate(validation_loader):\n",
    "        batch_Sp = data[0].unsqueeze(1)\n",
    "        batch_label= data[1]\n",
    "        \n",
    "        # you don't need to calculate the backward pass and the gradients during validation\n",
    "        # so you can call torch.no_grad() to only calculate the forward pass to save time and memory\n",
    "        with torch.no_grad():\n",
    "        \n",
    "            output= model(batch_Sp)\n",
    "            output=torch.argmax(output[0],dim=1)\n",
    "\n",
    "            #print('o', output.shape)\n",
    "            # MSE as objective\n",
    "            #print('l',batch_label.shape)\n",
    "            loss = Accuracy(output, batch_label.long())\n",
    "        \n",
    "            validation_loss += loss.data.item()\n",
    "    \n",
    "    validation_loss /= (batch_idx+1)\n",
    "    print('    | end of validation epoch {:3d} | time: {:5.2f}s | accs {:5.4f} |'.format(\n",
    "            epoch, (time.time() - start_time), validation_loss))\n",
    "    print('-' * 99)\n",
    "    \n",
    "    return validation_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |    15/   63 batches | ms/batch 176.06 | BCE 3.9167 |\n",
      "| epoch   1 |    30/   63 batches | ms/batch 174.15 | BCE 3.9134 |\n",
      "| epoch   1 |    45/   63 batches | ms/batch 173.48 | BCE 3.9147 |\n",
      "| epoch   1 |    60/   63 batches | ms/batch 173.16 | BCE 3.9156 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 10.83s | BCE 3.9161 |\n",
      "    | end of validation epoch   1 | time:  0.81s | accs 2.6316 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch   2 |    15/   63 batches | ms/batch 173.82 | BCE 3.9112 |\n",
      "| epoch   2 |    30/   63 batches | ms/batch 173.50 | BCE 3.9081 |\n",
      "| epoch   2 |    45/   63 batches | ms/batch 173.29 | BCE 3.9059 |\n",
      "| epoch   2 |    60/   63 batches | ms/batch 172.90 | BCE 3.9042 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   2 | time: 10.82s | BCE 3.9043 |\n",
      "    | end of validation epoch   2 | time:  0.80s | accs 3.9474 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch   3 |    15/   63 batches | ms/batch 173.28 | BCE 3.8709 |\n",
      "| epoch   3 |    30/   63 batches | ms/batch 172.58 | BCE 3.8690 |\n",
      "| epoch   3 |    45/   63 batches | ms/batch 172.64 | BCE 3.8674 |\n",
      "| epoch   3 |    60/   63 batches | ms/batch 172.56 | BCE 3.8640 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   3 | time: 10.80s | BCE 3.8626 |\n",
      "    | end of validation epoch   3 | time:  0.81s | accs 5.2632 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch   4 |    15/   63 batches | ms/batch 176.35 | BCE 3.8209 |\n",
      "| epoch   4 |    30/   63 batches | ms/batch 177.62 | BCE 3.8186 |\n",
      "| epoch   4 |    45/   63 batches | ms/batch 176.06 | BCE 3.8153 |\n",
      "| epoch   4 |    60/   63 batches | ms/batch 174.93 | BCE 3.8129 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   4 | time: 10.94s | BCE 3.8118 |\n",
      "    | end of validation epoch   4 | time:  0.84s | accs 6.7982 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch   5 |    15/   63 batches | ms/batch 181.84 | BCE 3.7729 |\n",
      "| epoch   5 |    30/   63 batches | ms/batch 178.26 | BCE 3.7798 |\n",
      "| epoch   5 |    45/   63 batches | ms/batch 176.70 | BCE 3.7764 |\n",
      "| epoch   5 |    60/   63 batches | ms/batch 176.64 | BCE 3.7760 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   5 | time: 11.05s | BCE 3.7770 |\n",
      "    | end of validation epoch   5 | time:  0.80s | accs 11.1842 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch   6 |    15/   63 batches | ms/batch 173.19 | BCE 3.7659 |\n",
      "| epoch   6 |    30/   63 batches | ms/batch 175.61 | BCE 3.7499 |\n",
      "| epoch   6 |    45/   63 batches | ms/batch 175.53 | BCE 3.7465 |\n",
      "| epoch   6 |    60/   63 batches | ms/batch 175.29 | BCE 3.7438 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   6 | time: 10.97s | BCE 3.7425 |\n",
      "    | end of validation epoch   6 | time:  0.81s | accs 12.0614 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch   7 |    15/   63 batches | ms/batch 175.56 | BCE 3.7015 |\n",
      "| epoch   7 |    30/   63 batches | ms/batch 175.16 | BCE 3.7066 |\n",
      "| epoch   7 |    45/   63 batches | ms/batch 174.87 | BCE 3.7086 |\n",
      "| epoch   7 |    60/   63 batches | ms/batch 174.84 | BCE 3.7105 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   7 | time: 10.95s | BCE 3.7081 |\n",
      "    | end of validation epoch   7 | time:  0.81s | accs 16.6667 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch   8 |    15/   63 batches | ms/batch 173.98 | BCE 3.6836 |\n",
      "| epoch   8 |    30/   63 batches | ms/batch 174.77 | BCE 3.6772 |\n",
      "| epoch   8 |    45/   63 batches | ms/batch 174.73 | BCE 3.6787 |\n",
      "| epoch   8 |    60/   63 batches | ms/batch 174.66 | BCE 3.6784 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   8 | time: 10.94s | BCE 3.6772 |\n",
      "    | end of validation epoch   8 | time:  0.81s | accs 18.6404 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch   9 |    15/   63 batches | ms/batch 174.63 | BCE 3.6503 |\n",
      "| epoch   9 |    30/   63 batches | ms/batch 174.00 | BCE 3.6489 |\n",
      "| epoch   9 |    45/   63 batches | ms/batch 174.22 | BCE 3.6509 |\n",
      "| epoch   9 |    60/   63 batches | ms/batch 173.84 | BCE 3.6500 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   9 | time: 10.89s | BCE 3.6503 |\n",
      "    | end of validation epoch   9 | time:  0.80s | accs 20.1754 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  10 |    15/   63 batches | ms/batch 172.44 | BCE 3.6382 |\n",
      "| epoch  10 |    30/   63 batches | ms/batch 173.39 | BCE 3.6327 |\n",
      "| epoch  10 |    45/   63 batches | ms/batch 173.10 | BCE 3.6281 |\n",
      "| epoch  10 |    60/   63 batches | ms/batch 173.08 | BCE 3.6250 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  10 | time: 10.84s | BCE 3.6245 |\n",
      "    | end of validation epoch  10 | time:  0.81s | accs 25.6579 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  11 |    15/   63 batches | ms/batch 172.37 | BCE 3.6080 |\n",
      "| epoch  11 |    30/   63 batches | ms/batch 172.28 | BCE 3.6003 |\n",
      "| epoch  11 |    45/   63 batches | ms/batch 172.29 | BCE 3.5976 |\n",
      "| epoch  11 |    60/   63 batches | ms/batch 173.04 | BCE 3.5952 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  11 | time: 10.86s | BCE 3.5933 |\n",
      "    | end of validation epoch  11 | time:  0.88s | accs 25.2193 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  12 |    15/   63 batches | ms/batch 182.23 | BCE 3.5790 |\n",
      "| epoch  12 |    30/   63 batches | ms/batch 178.59 | BCE 3.5720 |\n",
      "| epoch  12 |    45/   63 batches | ms/batch 178.49 | BCE 3.5719 |\n",
      "| epoch  12 |    60/   63 batches | ms/batch 178.27 | BCE 3.5681 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  12 | time: 11.16s | BCE 3.5665 |\n",
      "    | end of validation epoch  12 | time:  0.80s | accs 28.9474 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  13 |    15/   63 batches | ms/batch 177.11 | BCE 3.5500 |\n",
      "| epoch  13 |    30/   63 batches | ms/batch 177.07 | BCE 3.5450 |\n",
      "| epoch  13 |    45/   63 batches | ms/batch 178.84 | BCE 3.5375 |\n",
      "| epoch  13 |    60/   63 batches | ms/batch 179.99 | BCE 3.5390 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  13 | time: 11.25s | BCE 3.5413 |\n",
      "    | end of validation epoch  13 | time:  0.91s | accs 34.4298 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  14 |    15/   63 batches | ms/batch 189.41 | BCE 3.5315 |\n",
      "| epoch  14 |    30/   63 batches | ms/batch 187.35 | BCE 3.5200 |\n",
      "| epoch  14 |    45/   63 batches | ms/batch 186.95 | BCE 3.5177 |\n",
      "| epoch  14 |    60/   63 batches | ms/batch 186.79 | BCE 3.5178 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  14 | time: 11.68s | BCE 3.5143 |\n",
      "    | end of validation epoch  14 | time:  0.89s | accs 35.5263 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  15 |    15/   63 batches | ms/batch 188.75 | BCE 3.4944 |\n",
      "| epoch  15 |    30/   63 batches | ms/batch 188.05 | BCE 3.4969 |\n",
      "| epoch  15 |    45/   63 batches | ms/batch 184.09 | BCE 3.4945 |\n",
      "| epoch  15 |    60/   63 batches | ms/batch 183.05 | BCE 3.4923 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  15 | time: 11.44s | BCE 3.4918 |\n",
      "    | end of validation epoch  15 | time:  0.84s | accs 36.1842 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  16 |    15/   63 batches | ms/batch 180.21 | BCE 3.4811 |\n",
      "| epoch  16 |    30/   63 batches | ms/batch 183.15 | BCE 3.4642 |\n",
      "| epoch  16 |    45/   63 batches | ms/batch 184.02 | BCE 3.4606 |\n",
      "| epoch  16 |    60/   63 batches | ms/batch 183.16 | BCE 3.4619 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  16 | time: 11.46s | BCE 3.4605 |\n",
      "    | end of validation epoch  16 | time:  0.84s | accs 38.1579 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  17 |    15/   63 batches | ms/batch 184.97 | BCE 3.4452 |\n",
      "| epoch  17 |    30/   63 batches | ms/batch 181.33 | BCE 3.4391 |\n",
      "| epoch  17 |    45/   63 batches | ms/batch 179.86 | BCE 3.4377 |\n",
      "| epoch  17 |    60/   63 batches | ms/batch 180.05 | BCE 3.4354 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  17 | time: 11.26s | BCE 3.4343 |\n",
      "    | end of validation epoch  17 | time:  0.80s | accs 39.4737 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  18 |    15/   63 batches | ms/batch 176.82 | BCE 3.4071 |\n",
      "| epoch  18 |    30/   63 batches | ms/batch 178.06 | BCE 3.4052 |\n",
      "| epoch  18 |    45/   63 batches | ms/batch 177.66 | BCE 3.4008 |\n",
      "| epoch  18 |    60/   63 batches | ms/batch 177.99 | BCE 3.4043 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  18 | time: 11.13s | BCE 3.4052 |\n",
      "    | end of validation epoch  18 | time:  0.83s | accs 46.0526 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  19 |    15/   63 batches | ms/batch 182.04 | BCE 3.3929 |\n",
      "| epoch  19 |    30/   63 batches | ms/batch 181.05 | BCE 3.3884 |\n",
      "| epoch  19 |    45/   63 batches | ms/batch 180.33 | BCE 3.3901 |\n",
      "| epoch  19 |    60/   63 batches | ms/batch 180.29 | BCE 3.3812 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  19 | time: 11.27s | BCE 3.3834 |\n",
      "    | end of validation epoch  19 | time:  0.81s | accs 44.2982 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  20 |    15/   63 batches | ms/batch 179.01 | BCE 3.3565 |\n",
      "| epoch  20 |    30/   63 batches | ms/batch 179.69 | BCE 3.3557 |\n",
      "| epoch  20 |    45/   63 batches | ms/batch 180.68 | BCE 3.3569 |\n",
      "| epoch  20 |    60/   63 batches | ms/batch 180.72 | BCE 3.3560 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  20 | time: 11.30s | BCE 3.3551 |\n",
      "    | end of validation epoch  20 | time:  0.84s | accs 42.1053 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  21 |    15/   63 batches | ms/batch 185.11 | BCE 3.3313 |\n",
      "| epoch  21 |    30/   63 batches | ms/batch 181.82 | BCE 3.3361 |\n",
      "| epoch  21 |    45/   63 batches | ms/batch 179.80 | BCE 3.3325 |\n",
      "| epoch  21 |    60/   63 batches | ms/batch 180.46 | BCE 3.3289 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  21 | time: 11.28s | BCE 3.3264 |\n",
      "    | end of validation epoch  21 | time:  0.82s | accs 50.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  22 |    15/   63 batches | ms/batch 181.81 | BCE 3.3077 |\n",
      "| epoch  22 |    30/   63 batches | ms/batch 183.38 | BCE 3.3027 |\n",
      "| epoch  22 |    45/   63 batches | ms/batch 182.79 | BCE 3.2969 |\n",
      "| epoch  22 |    60/   63 batches | ms/batch 182.17 | BCE 3.2955 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  22 | time: 11.39s | BCE 3.2972 |\n",
      "    | end of validation epoch  22 | time:  0.85s | accs 50.6579 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  23 |    15/   63 batches | ms/batch 181.07 | BCE 3.2837 |\n",
      "| epoch  23 |    30/   63 batches | ms/batch 178.92 | BCE 3.2844 |\n",
      "| epoch  23 |    45/   63 batches | ms/batch 182.27 | BCE 3.2731 |\n",
      "| epoch  23 |    60/   63 batches | ms/batch 185.04 | BCE 3.2721 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  23 | time: 11.57s | BCE 3.2698 |\n",
      "    | end of validation epoch  23 | time:  0.87s | accs 48.6842 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  24 |    15/   63 batches | ms/batch 191.02 | BCE 3.2487 |\n",
      "| epoch  24 |    30/   63 batches | ms/batch 191.90 | BCE 3.2414 |\n",
      "| epoch  24 |    45/   63 batches | ms/batch 192.85 | BCE 3.2414 |\n",
      "| epoch  24 |    60/   63 batches | ms/batch 191.70 | BCE 3.2449 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  24 | time: 11.99s | BCE 3.2443 |\n",
      "    | end of validation epoch  24 | time:  0.87s | accs 55.2632 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  25 |    15/   63 batches | ms/batch 190.39 | BCE 3.2145 |\n",
      "| epoch  25 |    30/   63 batches | ms/batch 186.92 | BCE 3.2193 |\n",
      "| epoch  25 |    45/   63 batches | ms/batch 186.20 | BCE 3.2198 |\n",
      "| epoch  25 |    60/   63 batches | ms/batch 185.98 | BCE 3.2163 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  25 | time: 11.64s | BCE 3.2154 |\n",
      "    | end of validation epoch  25 | time:  0.86s | accs 53.2895 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  26 |    15/   63 batches | ms/batch 186.03 | BCE 3.1748 |\n",
      "| epoch  26 |    30/   63 batches | ms/batch 186.69 | BCE 3.1836 |\n",
      "| epoch  26 |    45/   63 batches | ms/batch 186.64 | BCE 3.1886 |\n",
      "| epoch  26 |    60/   63 batches | ms/batch 186.20 | BCE 3.1906 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  26 | time: 11.63s | BCE 3.1925 |\n",
      "    | end of validation epoch  26 | time:  0.84s | accs 53.9474 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  27 |    15/   63 batches | ms/batch 184.53 | BCE 3.1708 |\n",
      "| epoch  27 |    30/   63 batches | ms/batch 183.76 | BCE 3.1689 |\n",
      "| epoch  27 |    45/   63 batches | ms/batch 184.50 | BCE 3.1667 |\n",
      "| epoch  27 |    60/   63 batches | ms/batch 183.66 | BCE 3.1657 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  27 | time: 11.49s | BCE 3.1660 |\n",
      "    | end of validation epoch  27 | time:  0.84s | accs 63.1579 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  28 |    15/   63 batches | ms/batch 179.01 | BCE 3.1357 |\n",
      "| epoch  28 |    30/   63 batches | ms/batch 179.20 | BCE 3.1401 |\n",
      "| epoch  28 |    45/   63 batches | ms/batch 179.90 | BCE 3.1434 |\n",
      "| epoch  28 |    60/   63 batches | ms/batch 181.49 | BCE 3.1346 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  28 | time: 11.37s | BCE 3.1334 |\n",
      "    | end of validation epoch  28 | time:  0.87s | accs 63.5965 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  29 |    15/   63 batches | ms/batch 186.73 | BCE 3.1230 |\n",
      "| epoch  29 |    30/   63 batches | ms/batch 186.52 | BCE 3.1177 |\n",
      "| epoch  29 |    45/   63 batches | ms/batch 184.86 | BCE 3.1100 |\n",
      "| epoch  29 |    60/   63 batches | ms/batch 184.42 | BCE 3.1072 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  29 | time: 11.54s | BCE 3.1073 |\n",
      "    | end of validation epoch  29 | time:  0.85s | accs 64.6930 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  30 |    15/   63 batches | ms/batch 185.23 | BCE 3.0921 |\n",
      "| epoch  30 |    30/   63 batches | ms/batch 183.54 | BCE 3.0848 |\n",
      "| epoch  30 |    45/   63 batches | ms/batch 181.24 | BCE 3.0830 |\n",
      "| epoch  30 |    60/   63 batches | ms/batch 180.92 | BCE 3.0789 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  30 | time: 11.30s | BCE 3.0760 |\n",
      "    | end of validation epoch  30 | time:  0.85s | accs 66.2281 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  31 |    15/   63 batches | ms/batch 175.96 | BCE 3.0436 |\n",
      "| epoch  31 |    30/   63 batches | ms/batch 177.85 | BCE 3.0615 |\n",
      "| epoch  31 |    45/   63 batches | ms/batch 178.73 | BCE 3.0565 |\n",
      "| epoch  31 |    60/   63 batches | ms/batch 180.43 | BCE 3.0485 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  31 | time: 11.30s | BCE 3.0470 |\n",
      "    | end of validation epoch  31 | time:  0.86s | accs 66.0088 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  32 |    15/   63 batches | ms/batch 181.68 | BCE 3.0520 |\n",
      "| epoch  32 |    30/   63 batches | ms/batch 181.33 | BCE 3.0424 |\n",
      "| epoch  32 |    45/   63 batches | ms/batch 181.74 | BCE 3.0236 |\n",
      "| epoch  32 |    60/   63 batches | ms/batch 182.44 | BCE 3.0204 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  32 | time: 11.40s | BCE 3.0215 |\n",
      "    | end of validation epoch  32 | time:  0.79s | accs 67.7632 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  33 |    15/   63 batches | ms/batch 174.10 | BCE 2.9958 |\n",
      "| epoch  33 |    30/   63 batches | ms/batch 178.31 | BCE 3.0062 |\n",
      "| epoch  33 |    45/   63 batches | ms/batch 179.86 | BCE 2.9988 |\n",
      "| epoch  33 |    60/   63 batches | ms/batch 180.85 | BCE 2.9951 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  33 | time: 11.30s | BCE 2.9933 |\n",
      "    | end of validation epoch  33 | time:  0.82s | accs 69.0789 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  34 |    15/   63 batches | ms/batch 183.57 | BCE 2.9666 |\n",
      "| epoch  34 |    30/   63 batches | ms/batch 183.15 | BCE 2.9685 |\n",
      "| epoch  34 |    45/   63 batches | ms/batch 183.02 | BCE 2.9662 |\n",
      "| epoch  34 |    60/   63 batches | ms/batch 183.28 | BCE 2.9661 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  34 | time: 11.45s | BCE 2.9674 |\n",
      "    | end of validation epoch  34 | time:  0.81s | accs 66.8860 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  35 |    15/   63 batches | ms/batch 174.75 | BCE 2.9604 |\n",
      "| epoch  35 |    30/   63 batches | ms/batch 179.01 | BCE 2.9516 |\n",
      "| epoch  35 |    45/   63 batches | ms/batch 181.51 | BCE 2.9490 |\n",
      "| epoch  35 |    60/   63 batches | ms/batch 182.46 | BCE 2.9392 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  35 | time: 11.42s | BCE 2.9378 |\n",
      "    | end of validation epoch  35 | time:  0.83s | accs 69.7368 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  36 |    15/   63 batches | ms/batch 184.44 | BCE 2.9072 |\n",
      "| epoch  36 |    30/   63 batches | ms/batch 184.56 | BCE 2.9070 |\n",
      "| epoch  36 |    45/   63 batches | ms/batch 184.08 | BCE 2.9091 |\n",
      "| epoch  36 |    60/   63 batches | ms/batch 184.92 | BCE 2.9070 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  36 | time: 11.57s | BCE 2.9083 |\n",
      "    | end of validation epoch  36 | time:  0.87s | accs 71.7105 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  37 |    15/   63 batches | ms/batch 186.02 | BCE 2.8978 |\n",
      "| epoch  37 |    30/   63 batches | ms/batch 181.63 | BCE 2.8850 |\n",
      "| epoch  37 |    45/   63 batches | ms/batch 183.11 | BCE 2.8750 |\n",
      "| epoch  37 |    60/   63 batches | ms/batch 184.53 | BCE 2.8784 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  37 | time: 11.56s | BCE 2.8785 |\n",
      "    | end of validation epoch  37 | time:  0.89s | accs 73.9035 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  38 |    15/   63 batches | ms/batch 188.25 | BCE 2.8457 |\n",
      "| epoch  38 |    30/   63 batches | ms/batch 188.53 | BCE 2.8513 |\n",
      "| epoch  38 |    45/   63 batches | ms/batch 188.47 | BCE 2.8563 |\n",
      "| epoch  38 |    60/   63 batches | ms/batch 188.43 | BCE 2.8554 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  38 | time: 11.79s | BCE 2.8519 |\n",
      "    | end of validation epoch  38 | time:  0.84s | accs 73.9035 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  39 |    15/   63 batches | ms/batch 185.63 | BCE 2.8214 |\n",
      "| epoch  39 |    30/   63 batches | ms/batch 184.93 | BCE 2.8247 |\n",
      "| epoch  39 |    45/   63 batches | ms/batch 182.67 | BCE 2.8241 |\n",
      "| epoch  39 |    60/   63 batches | ms/batch 181.53 | BCE 2.8242 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  39 | time: 11.35s | BCE 2.8227 |\n",
      "    | end of validation epoch  39 | time:  0.81s | accs 75.4386 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  40 |    15/   63 batches | ms/batch 179.81 | BCE 2.8063 |\n",
      "| epoch  40 |    30/   63 batches | ms/batch 179.46 | BCE 2.7986 |\n",
      "| epoch  40 |    45/   63 batches | ms/batch 179.09 | BCE 2.8028 |\n",
      "| epoch  40 |    60/   63 batches | ms/batch 178.69 | BCE 2.7950 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  40 | time: 11.17s | BCE 2.7946 |\n",
      "    | end of validation epoch  40 | time:  0.80s | accs 77.1930 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  41 |    15/   63 batches | ms/batch 175.30 | BCE 2.7438 |\n",
      "| epoch  41 |    30/   63 batches | ms/batch 175.81 | BCE 2.7638 |\n",
      "| epoch  41 |    45/   63 batches | ms/batch 176.71 | BCE 2.7620 |\n",
      "| epoch  41 |    60/   63 batches | ms/batch 177.03 | BCE 2.7715 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  41 | time: 11.07s | BCE 2.7685 |\n",
      "    | end of validation epoch  41 | time:  0.80s | accs 78.5088 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  42 |    15/   63 batches | ms/batch 177.50 | BCE 2.7491 |\n",
      "| epoch  42 |    30/   63 batches | ms/batch 176.15 | BCE 2.7592 |\n",
      "| epoch  42 |    45/   63 batches | ms/batch 176.00 | BCE 2.7539 |\n",
      "| epoch  42 |    60/   63 batches | ms/batch 176.40 | BCE 2.7472 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  42 | time: 11.04s | BCE 2.7426 |\n",
      "    | end of validation epoch  42 | time:  0.80s | accs 79.1667 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  43 |    15/   63 batches | ms/batch 174.47 | BCE 2.7125 |\n",
      "| epoch  43 |    30/   63 batches | ms/batch 175.52 | BCE 2.7067 |\n",
      "| epoch  43 |    45/   63 batches | ms/batch 175.67 | BCE 2.7109 |\n",
      "| epoch  43 |    60/   63 batches | ms/batch 175.58 | BCE 2.7159 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  43 | time: 10.98s | BCE 2.7149 |\n",
      "    | end of validation epoch  43 | time:  0.80s | accs 79.1667 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  44 |    15/   63 batches | ms/batch 174.28 | BCE 2.7008 |\n",
      "| epoch  44 |    30/   63 batches | ms/batch 174.58 | BCE 2.6855 |\n",
      "| epoch  44 |    45/   63 batches | ms/batch 175.37 | BCE 2.6872 |\n",
      "| epoch  44 |    60/   63 batches | ms/batch 175.70 | BCE 2.6837 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  44 | time: 10.98s | BCE 2.6815 |\n",
      "    | end of validation epoch  44 | time:  0.81s | accs 80.7018 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  45 |    15/   63 batches | ms/batch 176.61 | BCE 2.6637 |\n",
      "| epoch  45 |    30/   63 batches | ms/batch 176.75 | BCE 2.6516 |\n",
      "| epoch  45 |    45/   63 batches | ms/batch 176.37 | BCE 2.6474 |\n",
      "| epoch  45 |    60/   63 batches | ms/batch 176.36 | BCE 2.6561 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  45 | time: 11.03s | BCE 2.6566 |\n",
      "    | end of validation epoch  45 | time:  0.80s | accs 81.1404 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  46 |    15/   63 batches | ms/batch 181.86 | BCE 2.6239 |\n",
      "| epoch  46 |    30/   63 batches | ms/batch 179.75 | BCE 2.6274 |\n",
      "| epoch  46 |    45/   63 batches | ms/batch 182.31 | BCE 2.6220 |\n",
      "| epoch  46 |    60/   63 batches | ms/batch 181.85 | BCE 2.6281 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  46 | time: 11.36s | BCE 2.6296 |\n",
      "    | end of validation epoch  46 | time:  0.80s | accs 81.7982 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  47 |    15/   63 batches | ms/batch 176.03 | BCE 2.5919 |\n",
      "| epoch  47 |    30/   63 batches | ms/batch 182.03 | BCE 2.5921 |\n",
      "| epoch  47 |    45/   63 batches | ms/batch 184.58 | BCE 2.5970 |\n",
      "| epoch  47 |    60/   63 batches | ms/batch 185.10 | BCE 2.5977 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  47 | time: 11.56s | BCE 2.5978 |\n",
      "    | end of validation epoch  47 | time:  0.82s | accs 79.6053 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  48 |    15/   63 batches | ms/batch 176.90 | BCE 2.5555 |\n",
      "| epoch  48 |    30/   63 batches | ms/batch 180.85 | BCE 2.5665 |\n",
      "| epoch  48 |    45/   63 batches | ms/batch 182.53 | BCE 2.5672 |\n",
      "| epoch  48 |    60/   63 batches | ms/batch 183.61 | BCE 2.5693 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  48 | time: 11.48s | BCE 2.5691 |\n",
      "    | end of validation epoch  48 | time:  0.89s | accs 80.4825 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  49 |    15/   63 batches | ms/batch 183.16 | BCE 2.5416 |\n",
      "| epoch  49 |    30/   63 batches | ms/batch 184.48 | BCE 2.5475 |\n",
      "| epoch  49 |    45/   63 batches | ms/batch 185.93 | BCE 2.5455 |\n",
      "| epoch  49 |    60/   63 batches | ms/batch 184.76 | BCE 2.5442 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  49 | time: 11.54s | BCE 2.5422 |\n",
      "    | end of validation epoch  49 | time:  0.81s | accs 83.7719 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  50 |    15/   63 batches | ms/batch 189.96 | BCE 2.5146 |\n",
      "| epoch  50 |    30/   63 batches | ms/batch 190.29 | BCE 2.5190 |\n",
      "| epoch  50 |    45/   63 batches | ms/batch 190.43 | BCE 2.5240 |\n",
      "| epoch  50 |    60/   63 batches | ms/batch 189.54 | BCE 2.5146 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  50 | time: 11.85s | BCE 2.5166 |\n",
      "    | end of validation epoch  50 | time:  0.87s | accs 82.4561 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  51 |    15/   63 batches | ms/batch 190.34 | BCE 2.4934 |\n",
      "| epoch  51 |    30/   63 batches | ms/batch 189.70 | BCE 2.4856 |\n",
      "| epoch  51 |    45/   63 batches | ms/batch 190.04 | BCE 2.4827 |\n",
      "| epoch  51 |    60/   63 batches | ms/batch 190.86 | BCE 2.4857 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  51 | time: 11.94s | BCE 2.4865 |\n",
      "    | end of validation epoch  51 | time:  0.93s | accs 85.0877 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  52 |    15/   63 batches | ms/batch 189.09 | BCE 2.4723 |\n",
      "| epoch  52 |    30/   63 batches | ms/batch 188.76 | BCE 2.4620 |\n",
      "| epoch  52 |    45/   63 batches | ms/batch 187.79 | BCE 2.4618 |\n",
      "| epoch  52 |    60/   63 batches | ms/batch 188.14 | BCE 2.4619 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  52 | time: 11.76s | BCE 2.4611 |\n",
      "    | end of validation epoch  52 | time:  0.91s | accs 83.7719 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  53 |    15/   63 batches | ms/batch 183.19 | BCE 2.4573 |\n",
      "| epoch  53 |    30/   63 batches | ms/batch 184.68 | BCE 2.4471 |\n",
      "| epoch  53 |    45/   63 batches | ms/batch 185.14 | BCE 2.4449 |\n",
      "| epoch  53 |    60/   63 batches | ms/batch 185.95 | BCE 2.4348 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  53 | time: 11.64s | BCE 2.4325 |\n",
      "    | end of validation epoch  53 | time:  0.83s | accs 83.1140 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  54 |    15/   63 batches | ms/batch 183.38 | BCE 2.3910 |\n",
      "| epoch  54 |    30/   63 batches | ms/batch 187.19 | BCE 2.3949 |\n",
      "| epoch  54 |    45/   63 batches | ms/batch 188.33 | BCE 2.4027 |\n",
      "| epoch  54 |    60/   63 batches | ms/batch 188.36 | BCE 2.4046 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  54 | time: 11.78s | BCE 2.4055 |\n",
      "    | end of validation epoch  54 | time:  0.86s | accs 83.7719 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  55 |    15/   63 batches | ms/batch 187.15 | BCE 2.3926 |\n",
      "| epoch  55 |    30/   63 batches | ms/batch 187.02 | BCE 2.3872 |\n",
      "| epoch  55 |    45/   63 batches | ms/batch 187.76 | BCE 2.3801 |\n",
      "| epoch  55 |    60/   63 batches | ms/batch 186.98 | BCE 2.3783 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  55 | time: 11.68s | BCE 2.3755 |\n",
      "    | end of validation epoch  55 | time:  0.85s | accs 84.4298 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  56 |    15/   63 batches | ms/batch 182.00 | BCE 2.3450 |\n",
      "| epoch  56 |    30/   63 batches | ms/batch 188.05 | BCE 2.3530 |\n",
      "| epoch  56 |    45/   63 batches | ms/batch 189.56 | BCE 2.3519 |\n",
      "| epoch  56 |    60/   63 batches | ms/batch 188.59 | BCE 2.3521 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  56 | time: 11.79s | BCE 2.3505 |\n",
      "    | end of validation epoch  56 | time:  0.86s | accs 83.1140 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  57 |    15/   63 batches | ms/batch 184.54 | BCE 2.3384 |\n",
      "| epoch  57 |    30/   63 batches | ms/batch 183.39 | BCE 2.3323 |\n",
      "| epoch  57 |    45/   63 batches | ms/batch 184.38 | BCE 2.3275 |\n",
      "| epoch  57 |    60/   63 batches | ms/batch 184.81 | BCE 2.3222 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  57 | time: 11.56s | BCE 2.3257 |\n",
      "    | end of validation epoch  57 | time:  0.84s | accs 87.2807 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  58 |    15/   63 batches | ms/batch 179.82 | BCE 2.3039 |\n",
      "| epoch  58 |    30/   63 batches | ms/batch 179.74 | BCE 2.3088 |\n",
      "| epoch  58 |    45/   63 batches | ms/batch 180.18 | BCE 2.2994 |\n",
      "| epoch  58 |    60/   63 batches | ms/batch 180.89 | BCE 2.2975 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  58 | time: 11.29s | BCE 2.2967 |\n",
      "    | end of validation epoch  58 | time:  0.79s | accs 87.9386 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  59 |    15/   63 batches | ms/batch 178.68 | BCE 2.2898 |\n",
      "| epoch  59 |    30/   63 batches | ms/batch 176.42 | BCE 2.2721 |\n",
      "| epoch  59 |    45/   63 batches | ms/batch 175.56 | BCE 2.2679 |\n",
      "| epoch  59 |    60/   63 batches | ms/batch 177.92 | BCE 2.2672 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  59 | time: 11.15s | BCE 2.2660 |\n",
      "    | end of validation epoch  59 | time:  0.86s | accs 87.7193 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  60 |    15/   63 batches | ms/batch 190.78 | BCE 2.2584 |\n",
      "| epoch  60 |    30/   63 batches | ms/batch 188.98 | BCE 2.2530 |\n",
      "| epoch  60 |    45/   63 batches | ms/batch 187.18 | BCE 2.2451 |\n",
      "| epoch  60 |    60/   63 batches | ms/batch 188.06 | BCE 2.2368 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  60 | time: 11.77s | BCE 2.2373 |\n",
      "    | end of validation epoch  60 | time:  0.85s | accs 87.7193 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "total_epoch = 60  # train the model for 100 epochs\n",
    "model_save = 'best_model_SV.pt'  # path to save the best validation model\n",
    "\n",
    "# main function\n",
    "\n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "\n",
    "model_AN = AlexNet()\n",
    "optimizer = optim.Adam(model_AN.parameters(), lr=1e-4)\n",
    "for epoch in range(1, total_epoch + 1):\n",
    "    training_loss.append(train(model_AN , epoch))\n",
    "    validation_loss.append(validate(model_AN, epoch))\n",
    "    if training_loss[-1] == np.min(training_loss):\n",
    "        print('      Best training model found.')\n",
    "    if validation_loss[-1] == np.max(validation_loss):\n",
    "        # save current best model on validation set\n",
    "        with open(model_save, 'wb') as f:\n",
    "            torch.save(model_AN.state_dict(), f)\n",
    "            print('      Best validation model found and saved.')\n",
    "    \n",
    "    print('-' * 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    | end of test |  test_accs:  86.0\n"
     ]
    }
   ],
   "source": [
    "# TODO: evaluate your best model on the test set\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_SV=AlexNet()\n",
    "model_SV.load_state_dict(torch.load('best_model_SV.pt'))\n",
    "test_loss=0\n",
    "for batch_idx, data in enumerate(test_loader):\n",
    "    \n",
    "    batch_Sp = data[0].unsqueeze(1)\n",
    "    batch_label= data[1]\n",
    "        \n",
    "        # you don't need to calculate the backward pass and the gradients during validation\n",
    "        # so you can call torch.no_grad() to only calculate the forward pass to save time and memory\n",
    "    with torch.no_grad():\n",
    "\n",
    "        output= model_SV(batch_Sp)\n",
    "        output=torch.argmax(output[0],dim=1)\n",
    "\n",
    "        #print('o', output.shape)\n",
    "        # MSE as objective\n",
    "        #print('l',batch_label.shape)\n",
    "        loss = Accuracy(output, batch_label.long())\n",
    "\n",
    "        test_loss += loss.data.item()\n",
    "    \n",
    "test_loss /= (batch_idx+1)\n",
    "print('    | end of test |  test_accs: ', test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the acoustic event detection task, the estimated target class can be directly used as what we want is indeed the class label. However, in speaker recognition task we have another step - **scoring**. This stage is useful here because in many cases the query speaker might not match any of the saved speakers, and we might not be able to assign a proper speaker label to it. In speaker identification task, this means that we cannot always perform *argmax* on the network output to assign the corresponding speaker label to it. In speaker verification task, this means that we can only say the query speaker matches the target speaker when the similarity score is higher than a (predefined) threshold. Acoustic event detection task often assumes that all the possible types of audio events are known and there won't be out-of-box event types, and there is also no need to perform such verification process for such scenes.\n",
    "\n",
    "One of the most simple way to perform scoring is to compare the speaker embedding from the query utterance to the overall characteristics from the training set. The overall characteristics for a target speaker is generated by averaging across all the speaker embeddings of the speaker's training utterances. You can intuitively treat it as a clustering process, and the average embedding is essentially the cluster center for all the possible speaker embeddings for one speaker.\n",
    "\n",
    "Extract all the training speaker embeddings below and calculate the average speaker embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 256]) torch.Size([500])\n"
     ]
    }
   ],
   "source": [
    "# TODO: extract speaker embeddings for each of the speaker's training utterance\n",
    "# use the best model on validation set above\n",
    "# save all training speaker embeddings as all_embedding (shape: (500, 256) for (num_spk*num_utterance, embedding_dim))\n",
    "# save average speaker embeddings as average_embedding (shape: (50, 256) for (num_spk, embedding_dim))\n",
    "# remember to convert them to numpy.array format!\n",
    "\n",
    "flag_all_embedding=0\n",
    "\n",
    "for batch_idx, data in enumerate(train_loader):\n",
    "    \n",
    "    batch_Sp = data[0].unsqueeze(1)\n",
    "    batch_label= data[1]\n",
    "        \n",
    "        # you don't need to calculate the backward pass and the gradients during validation\n",
    "        # so you can call torch.no_grad() to only calculate the forward pass to save time and memory\n",
    "    with torch.no_grad():\n",
    "        output= model_SV(batch_Sp)\n",
    "        embed=output[1]\n",
    "        if flag_all_embedding==0:\n",
    "            flag_all_embedding=1\n",
    "            all_embedding=embed\n",
    "            all_L=batch_label\n",
    "        else:\n",
    "            all_embedding=torch.cat((all_embedding,embed),0)\n",
    "            all_L=torch.cat((all_L,batch_label))\n",
    "            \n",
    "        #output=torch.argmax(output[0],dim=1)\n",
    "\n",
    "        #print('o', output.shape)\n",
    "        # MSE as objective\n",
    "        #print('l',batch_label.shape)\n",
    "        #loss = Accuracy(output, batch_label.long())\n",
    "\n",
    "        #test_loss += loss.data.item()\n",
    "print(all_embedding.size(), all_L.size())\n",
    "all_embedding=all_embedding.cpu().detach().numpy()\n",
    "all_L=all_L.cpu().detach().numpy()\n",
    "\n",
    "average_embedding=np.zeros((50,np.shape(all_embedding)[1]))\n",
    "for i in range(50):\n",
    "    average_embedding[i,:]=np.mean(all_embedding[all_L.astype('int')==i,:],axis=0)\n",
    "all_embedding=np.array(all_embedding)\n",
    "average_embedding=np.array(average_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500,)\n"
     ]
    }
   ],
   "source": [
    "print(all_L.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the embeddings. Note that the embeddings are pretty high-dimensional (256-dimensional) and it is impossible to directly visualize them. We need certain **dimension reduction** methods to map them to a low-dimensional space (2 or 3-dimensional space) for us to learn about their properties. The most simply way to perform dimension reduction is **principal component analysis (PCA)**, and let's use the [*sklearn.decomposition.PCA*](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) to do it.\n",
    "\n",
    "Apply the PCA function to the ***all_embedding*** matrix, set the number of reduced dimension to 2 (*n_components=2*), and visualize the embeddings for the first 5 speakers (first 50 embeddings in *all_embedding*) by plotting a scatter plot. Use different colors for different speakers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: perform PCA on the embeddings, visualize the embeddings for the first 5 speakers\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(all_embedding)\n",
    "\n",
    "y=pca.transform(all_embedding)\n",
    "\n",
    "\n",
    "data = pd.DataFrame({\"X Value\": y[:, 0], \"Y Value\":y[:, 1], \"Category\": all_L})\n",
    "groups = data.groupby(\"Category\")\n",
    "plt.figure(figsize=(10,10))\n",
    "for name, group in groups:\n",
    "    if name<5:\n",
    "\n",
    "\n",
    "        plt.scatter(group[\"X Value\"], group[\"Y Value\"], s=90, label=name)\n",
    "plt.legend()\n",
    "\n",
    "plt.title( 'all_embedding after PCS')\n",
    "    \n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that embeddings for different speakers are separate apart and each speaker has a cluster. For a query speaker utterance, we can compute its score with respect to all the speakers in the training utterances by calculating a similarity measure between the speaker embedding for this query utterance and all the average speaker embeddings. One of such similarity measure is **cosine similarity**, and let's apply it on our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: calculate the cosine similarity scores between each utterance in the test set and all the average embeddings\n",
    "# save your cosine similarity scores in cos_sim (shape: (50, 50) for (num_target_spk, num_test_utterance))\n",
    "\n",
    "from scipy import spatial\n",
    "cos_sim=np.zeros((50,50))\n",
    "ts_L=np.zeros(50)\n",
    "for batch_idx, data in enumerate(test_loader):\n",
    "    \n",
    "    batch_Sp = data[0].unsqueeze(1)\n",
    "    batch_label= data[1]\n",
    "        # you don't need to calculate the backward pass and the gradients during validation\n",
    "        # so you can call torch.no_grad() to only calculate the forward pass to save time and memory\n",
    "    ts_L[batch_idx]=batch_label.cpu().detach().numpy()\n",
    "    with torch.no_grad():\n",
    "        output= model_SV(batch_Sp)\n",
    "        embed=output[1]\n",
    "        for i in range(50):\n",
    "            result = 1 - spatial.distance.cosine(embed.cpu().detach().numpy(),average_embedding[i,:] )\n",
    "            cos_sim[i,batch_idx]=result\n",
    "order=[np.where(idd==ts_L.astype('int'))[0][0] for idd in range(50)]\n",
    "cos_sim=cos_sim[:,order]            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30, 0, 6, 5, 1, 29, 34, 28, 35, 17, 14, 41, 47, 46, 42, 13, 19, 9, 21, 11, 20, 49, 39, 40, 48, 22, 10, 18, 12, 32, 26, 4, 7, 8, 3, 27, 31, 25, 33, 45, 36, 16, 23, 24, 15, 37, 44, 38, 43, 2]\n"
     ]
    }
   ],
   "source": [
    "print(order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have a 50*50 matrix of all the cosine similarity scores. Let's visualize it as a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize the cosine similarity scores as a confusion matrix\n",
    "\n",
    "# this is the most simple way to plot it\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(cos_sim)\n",
    "plt.xlabel('Test utterance (speaker) index', fontsize=16)\n",
    "plt.ylabel('Train speaker index', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may observe that for a successfull speaker recognition, the elements on the diagonal typically present the highest score for each test utterance. This is indeed what we expect.\n",
    "\n",
    "Now we need a way to properly evaluate our system. We had the classification accuracy above which can be used for the identification task where we assume that we know all the target speakers to be identified in advance. For verification task which is indeed a binary classification task (accept/reject), we need other ways to determine the threshold for the similarity scores as well as measure the system's performance. One widely-used metric for evaluation in the verification task is the ***equal error rate (EER)***, defined as a value when [Type I = Type II error](https://en.wikipedia.org/wiki/Type_I_and_type_II_errors). It can be derived from the [***receiver operating characteristic (ROC) curve***](https://scikit-learn.org/stable/modules/model_evaluation.html#roc-metrics). I will not go into the details here and you can refer to the links for further information as well as its usage. A sample implementation of EER is directly provided here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall EER: 2.94%; decision threshold for similarity scores: 0.52\n"
     ]
    }
   ],
   "source": [
    "# EER calculation with ROC curve\n",
    "# adopted from https://yangcha.github.io/EER-ROC/\n",
    "\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "def EER(y, y_pred):\n",
    "    # y_pred is a list of similarity scores for the verification task (cosine similarity values)\n",
    "    # y is a list of the binary labels (accept/reject), where 1 is used for acceptance\n",
    "    fpr, tpr, threshold = roc_curve(y, y_pred, pos_label=1)\n",
    "    \n",
    "    eer = brentq(lambda x : 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "    decision_threshold = interp1d(fpr, threshold)(eer)\n",
    "    \n",
    "    return eer, decision_threshold\n",
    "\n",
    "# example usage on the test utterances\n",
    "# verify if each test utterance matches each of the target speakers\n",
    "# a total of 50*50=2500 verification tasks\n",
    "\n",
    "label = np.diag(np.ones(50)).reshape(-1).astype(np.float32)\n",
    "eer, decision_threshold = EER(label, cos_sim.reshape(-1))\n",
    "print('Overall EER: {:.2f}%; decision threshold for similarity scores: {:.2f}'.format(eer*100, decision_threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Siamese Network and Triplet Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we have implemented above is an example pipeline - we train a standard multiclass classification system, use the feature as speaker embeddings, and identify (classify) or verify (score) them for the two tasks. However, there are other ways that we can train the model. Here we introduce another pipeline called ***siamese network and triplet loss***.\n",
    "\n",
    "Let's first take a look at this illustration (https://levelup.gitconnected.com/metric-learning-using-siamese-and-triplet-convolutional-neural-networks-ed5b01d83be3).\n",
    "\n",
    "![](https://miro.medium.com/max/1400/1*MIPdyhJGx6uLiob9UI9S0w.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have three input images in this illustration. The term ***siamese network*** corresponds to the design that all three input images share the same network (same architecture and same parameters), and the term ***triplet loss*** corresponds to the training objective function applied to the three inputs. The three inputs, $x_a$, $x_p$ and $x_n$, represent the ***a*nchor input**, ***p*ositive sample**, and the ***n*egative sample**, respectively. In our case of speaker recognition, the anchor input is an utterance from a random speaker A, the positive sample is another utterance from the same speaker A, and the negative sample is an utterance from a random speaker B different from A. \n",
    "\n",
    "The triplet loss tries to **maximize** the similarity between the anchor input and the positive sample, and to **minimize** the similarity between the anchor input and the negative sample. The $\\alpha$ term is a predefined nonnegative scalar determining the minimum margin of the difference between the two similarity scores. In our case of speaker recognition where cosine similarity is used for the similarity score, it is equivalent to the following objective:\n",
    "$$\\theta_{a,p} > \\theta_{a,n} + \\alpha$$\n",
    "where $\\theta$ corresponds to the cosine similarity. The triplet loss can then be written as:\n",
    "$$L_{triplet} = max(\\theta_{a,n} - \\theta_{a,p} + \\alpha , 0)$$\n",
    "\n",
    "When $\\theta_{a,p} > \\theta_{a,n} + \\alpha$, $max(\\theta_{a,n} - \\theta_{a,p} + \\alpha , 0) = 0$ and there's no gradient with respect to the cosine similarity scores. This means that the negative and positive samples are already separated enough and no further optimization is required. When $\\theta_{a,p} \\leq \\theta_{a,n} + \\alpha$, $max(\\theta_{a,n} - \\theta_{a,p} + \\alpha , 0) = \\theta_{a,n} - \\theta_{a,p} + \\alpha$, and gradient descent on $L_{triplet}$ will minimize $\\theta_{a,n}$ and maximize $\\theta_{a,p}$. Given that the range of the cosine similarity scores are [-1, 1], let's empirically set $\\alpha=1$ in our experiment.\n",
    "\n",
    "Note that the difference between our loss and the loss in the illustration comes from the use of the **distance measure** $d(\\cdot)$ instead of a **similarity measure** in the illustration. If we use a distance measure here (e.g. Euclidean distance), then the loss needs to be modified accordingly.\n",
    "\n",
    "Now let's implement this triplet loss and add it to the original cross entropy loss. You need to write a new data loading function as we need one random positive sample and one negative sample for each of the anchor input in the training set. You can do something like:\n",
    "- Sample another batch of data for the triplet loss:\n",
    "    - Sample two speaker indices (e.g. speaker 2 and speaker 6).\n",
    "    - Sample two utterances from the first speaker (e.g speaker 2) to form the anchor input and the positive sample, and sample one utterance from the second speaker (speaker 6) to form the negative sample.\n",
    "    - Generate the speaker embeddings for the selected utterances, calculate cosine similarity and the triplet loss.\n",
    "\n",
    "Note that the validation stage also makes use of the randomly sampled utterances for triplet loss. If you want to use a fixed set of triplet input samples, you can predefine a set of speaker and utterance indices shared by all epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n"
     ]
    }
   ],
   "source": [
    "# TODO: train another network with triplet loss\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "batch_size = 8\n",
    "def findTrip(a,b,c,idx):\n",
    "    if a==b and a!=c:\n",
    "        pos=idx[0]\n",
    "        ref=idx[1]\n",
    "        neg=idx[2]\n",
    "    elif a==c and a!=b:\n",
    "        pos=idx[0]\n",
    "        ref=idx[2]\n",
    "        neg=idx[1]\n",
    "    elif b==c and b!=a:\n",
    "        pos=idx[1]\n",
    "        ref=idx[2]\n",
    "        neg=idx[0]\n",
    "    else:\n",
    "        pos=0\n",
    "        ref=0\n",
    "        neg=0\n",
    "    return ref,pos,neg\n",
    "\n",
    "# a class to load the saved h5py dataset\n",
    "class dataset_pipeline(Dataset):\n",
    "    def __init__(self, path):\n",
    "        super(dataset_pipeline, self).__init__()\n",
    "\n",
    "        self.h5pyLoader = h5py.File(path, 'r')\n",
    "        \n",
    "        self.spec = self.h5pyLoader['spec']\n",
    "        self.label=self.h5pyLoader['label']\n",
    "        \n",
    "        self.ITR= list(itertools.combinations(range(self.spec.shape[0]), 3))\n",
    "        self._len = len(self.ITR)  # number of utterances\n",
    "    def __getitem__(self, index):\n",
    "        a,b,c=self.ITR[index]\n",
    "        L1,L2,L3=self.label[a],self.label[b],self.label[c]\n",
    "        ref,pos,neg=findTrip(L1.astype('int'),L2.astype('int'),L3.astype('int'), (a,b,c))\n",
    "        spec_item_r, label_item_r, spec_item_p, label_item_p, spec_item_n, label_item_n=[],[],[],[],[],[]\n",
    "        if ref or pos or neg:\n",
    "            spec_item_r = torch.from_numpy(self.spec[ref].astype(np.float32))\n",
    "            label_item_r=self.label[ref].astype(np.float32)\n",
    "            spec_item_p = torch.from_numpy(self.spec[pos].astype(np.float32))\n",
    "            label_item_p=self.label[pos].astype(np.float32)\n",
    "            spec_item_n = torch.from_numpy(self.spec[neg].astype(np.float32))\n",
    "            label_item_n=self.label[neg].astype(np.float32)\n",
    "        #label_item = torch.from_numpy([self.label[index].astype(np.float32)])\n",
    "        return spec_item_r, label_item_r, spec_item_p, label_item_p, spec_item_n, label_item_n\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self._len\n",
    "    \n",
    "# define data loaders\n",
    "train_loader2 = DataLoader(dataset_pipeline('tr_set2.hdf5'), \n",
    "                          batch_size=1, \n",
    "                          shuffle=True,  # this ensures that the sequential order of the training samples will be shuffled for different training epochs\n",
    "                         )\n",
    "\n",
    "validation_loader2 = DataLoader(dataset_pipeline('val_set2.hdf5'), \n",
    "                               batch_size=1, \n",
    "                               shuffle=False,  # typically we fix the sequential order of the validation samples\n",
    "                              )\n",
    "test_loader2 = DataLoader(dataset_pipeline('test_set2.hdf5'), \n",
    "                               batch_size=1, \n",
    "                               shuffle=False,  # typically we fix the sequential order of the validation samples\n",
    "                              )\n",
    "dataset_len = len(train_loader)\n",
    "log_step = dataset_len // 4\n",
    "print(dataset_len)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossF = nn.CrossEntropyLoss()\n",
    "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "def train(model, epoch, versatile=True):\n",
    "    start_time = time.time()\n",
    "    model = model.train()  # set the model to training mode. Always do this before you start training!\n",
    "    train_loss = 0.\n",
    "    \n",
    "    # load batch data\n",
    "    for batch_idx, data in enumerate(train_loader2):\n",
    "        if len(data[0])>0:\n",
    "            batch_Sp_r = data[0].unsqueeze(1)\n",
    "            batch_label_r= data[1]\n",
    "            batch_Sp_p = data[2].unsqueeze(1)\n",
    "            batch_label_p= data[3]\n",
    "            batch_Sp_n = data[4].unsqueeze(1)\n",
    "            batch_label_n= data[5]\n",
    "            # clean up the gradients in the optimizer\n",
    "            # this should be called for each batch\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output_r= model(batch_Sp_r)\n",
    "            output_p= model(batch_Sp_p)\n",
    "            output_n= model(batch_Sp_n)\n",
    "\n",
    "            dp=1-cos(output_p[1],output_r[1] )\n",
    "            dn=1-cos(output_n[1],output_r[1] )\n",
    "\n",
    "            #output=torch.argmax(output,dim=1)\n",
    "\n",
    "            #print('o', output[0])\n",
    "            # MSE as objective\n",
    "            #print('l',batch_label)\n",
    "            loss = nn.functional.relu(dp-dn+1)\n",
    "\n",
    "            # automatically calculate the backward pass\n",
    "            loss.sum().backward()\n",
    "            # perform the actual backpropagation\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.sum().data.item()\n",
    "\n",
    "            # OPTIONAL: you can print the training progress \n",
    "            if versatile:\n",
    "                if (batch_idx+1) % log_step == 0:\n",
    "                    elapsed = time.time() - start_time\n",
    "                    print('| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | BCE {:5.4f} |'.format(\n",
    "                        epoch, batch_idx+1, len(train_loader),\n",
    "                        elapsed * 1000 / (batch_idx+1), \n",
    "                        train_loss / (batch_idx+1)\n",
    "                        ))\n",
    "\n",
    "            train_loss /= (batch_idx+1)\n",
    "            print('-' * 99)\n",
    "            print('    | end of training epoch {:3d} | time: {:5.2f}s | BCE {:5.4f} |'.format(\n",
    "                    epoch, (time.time() - start_time), train_loss))\n",
    "    \n",
    "    return train_loss\n",
    "        \n",
    "def validate(model, epoch):\n",
    "    start_time = time.time()\n",
    "    model = model.eval()  # set the model to evaluation mode. Always do this during validation or test phase!\n",
    "    validation_loss = 0.\n",
    "    \n",
    "    # load batch data\n",
    "    for batch_idx, data in enumerate(validation_loader2):\n",
    "        if len(data[0])>0:\n",
    "            batch_Sp_r = data[0].unsqueeze(1)\n",
    "            batch_label_r= data[1]\n",
    "            batch_Sp_p = data[2].unsqueeze(1)\n",
    "            batch_label_p= data[3]\n",
    "            batch_Sp_n = data[4].unsqueeze(1)\n",
    "            batch_label_n= data[5]\n",
    "            # clean up the gradients in the optimizer\n",
    "            # this should be called for each batch\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output_r= model(batch_Sp_r)\n",
    "            output_p= model(batch_Sp_p)\n",
    "            output_n= model(batch_Sp_n)\n",
    "\n",
    "            dp=1-cos(output_p[1],output_r[1])\n",
    "            dn=1-cos(output_n[1],output_r[1] )\n",
    "\n",
    "            #output=torch.argmax(output,dim=1)\n",
    "\n",
    "            #print('o', output[0])\n",
    "            # MSE as objective\n",
    "            #print('l',batch_label)\n",
    "            loss = nn.functional.relu(dp-dn+1)\n",
    "        \n",
    "            validation_loss += loss.data.item()\n",
    "    \n",
    "            validation_loss /= (batch_idx+1)\n",
    "            print('    | end of validation epoch {:3d} | time: {:5.2f}s | accs {:5.4f} |'.format(\n",
    "                    epoch, (time.time() - start_time), validation_loss))\n",
    "            print('-' * 99)\n",
    "    \n",
    "    return validation_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  1.69s | BCE 0.0278 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  1.77s | BCE 0.0223 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  1.87s | BCE 0.0112 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  1.97s | BCE 0.0089 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  2.05s | BCE 0.0085 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  2.14s | BCE 0.0079 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  2.22s | BCE 0.0076 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  2.31s | BCE 0.0069 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  2.42s | BCE 0.0050 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  2.50s | BCE 0.0050 |\n",
      "| epoch   1 |   225/   63 batches | ms/batch 11.54 | BCE 0.0045 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  2.60s | BCE 0.0045 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  2.68s | BCE 0.0043 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  2.77s | BCE 0.0041 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  2.87s | BCE 0.0035 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  2.95s | BCE 0.0035 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  3.04s | BCE 0.0033 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  3.13s | BCE 0.0033 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  3.24s | BCE 0.0030 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  3.34s | BCE 0.0028 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  3.43s | BCE 0.0027 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  3.51s | BCE 0.0027 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  3.61s | BCE 0.0025 |\n",
      "| epoch   1 |   405/   63 batches | ms/batch  9.15 | BCE 0.0025 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  3.70s | BCE 0.0025 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  3.80s | BCE 0.0024 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  3.90s | BCE 0.0022 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  4.00s | BCE 0.0022 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  4.10s | BCE 0.0022 |\n",
      "| epoch   1 |   480/   63 batches | ms/batch  8.77 | BCE 0.0020 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  4.21s | BCE 0.0020 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  4.33s | BCE 0.0019 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  4.43s | BCE 0.0017 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  4.53s | BCE 0.0016 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  4.63s | BCE 0.0016 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  4.73s | BCE 0.0015 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  4.84s | BCE 0.0014 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  4.94s | BCE 0.0014 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  5.03s | BCE 0.0012 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  5.11s | BCE 0.0015 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  5.21s | BCE 0.0014 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  5.31s | BCE 0.0014 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  5.41s | BCE 0.0014 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  5.50s | BCE 0.0013 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  5.60s | BCE 0.0014 |\n",
      "| epoch   1 |   780/   63 batches | ms/batch  7.29 | BCE 0.0012 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  5.68s | BCE 0.0012 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  5.80s | BCE 0.0011 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  5.89s | BCE 0.0012 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  6.00s | BCE 0.0012 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  6.09s | BCE 0.0015 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  6.19s | BCE 0.0010 |\n",
      "| epoch   1 |   930/   63 batches | ms/batch  6.77 | BCE 0.0011 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  6.29s | BCE 0.0011 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  6.38s | BCE 0.0011 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  6.50s | BCE 0.0011 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  6.60s | BCE 0.0010 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  6.69s | BCE 0.0009 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  6.78s | BCE 0.0009 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  6.89s | BCE 0.0008 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  6.97s | BCE 0.0010 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  7.07s | BCE 0.0008 |\n",
      "| epoch   1 |  1110/   63 batches | ms/batch  6.44 | BCE 0.0006 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  7.15s | BCE 0.0006 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  7.24s | BCE 0.0009 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  7.35s | BCE 0.0009 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  7.45s | BCE 0.0008 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  7.54s | BCE 0.0007 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  7.64s | BCE 0.0008 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  7.72s | BCE 0.0007 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  7.80s | BCE 0.0006 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  7.92s | BCE 0.0008 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  8.04s | BCE 0.0007 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  8.13s | BCE 0.0004 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  8.23s | BCE 0.0008 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  8.32s | BCE 0.0007 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  8.42s | BCE 0.0008 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  8.54s | BCE 0.0007 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  8.64s | BCE 0.0007 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  8.73s | BCE 0.0007 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  8.81s | BCE 0.0007 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  8.91s | BCE 0.0007 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  9.03s | BCE 0.0004 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  9.12s | BCE 0.0006 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  9.21s | BCE 0.0006 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  9.31s | BCE 0.0004 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  9.40s | BCE 0.0003 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  9.49s | BCE 0.0006 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  9.58s | BCE 0.0006 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  9.67s | BCE 0.0006 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  9.78s | BCE 0.0006 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  9.88s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  9.99s | BCE 0.0004 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 10.10s | BCE 0.0006 |\n",
      "| epoch   1 |  1785/   63 batches | ms/batch  5.72 | BCE 0.0006 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 10.20s | BCE 0.0006 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 10.34s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 10.43s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 10.52s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 10.62s | BCE 0.0005 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 10.75s | BCE 0.0003 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 10.87s | BCE 0.0004 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 10.97s | BCE 0.0005 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 11.06s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 11.19s | BCE 0.0006 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 11.29s | BCE 0.0004 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 11.39s | BCE 0.0000 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |  2205/   63 batches | ms/batch  5.21 | BCE 0.0004 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 11.48s | BCE 0.0004 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 11.56s | BCE 0.0005 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 11.65s | BCE 0.0004 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 11.74s | BCE 0.0004 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 11.84s | BCE 0.0004 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 11.93s | BCE 0.0004 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 12.01s | BCE 0.0004 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 12.11s | BCE 0.0005 |\n",
      "| epoch   1 |  2310/   63 batches | ms/batch  5.28 | BCE 0.0004 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 12.20s | BCE 0.0004 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 12.31s | BCE 0.0004 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 12.43s | BCE 0.0004 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 12.55s | BCE 0.0004 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 12.64s | BCE 0.0004 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 12.73s | BCE 0.0001 |\n",
      "| epoch   1 |  2475/   63 batches | ms/batch  5.18 | BCE 0.0004 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 12.82s | BCE 0.0004 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 12.91s | BCE 0.0004 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 12.98s | BCE 0.0004 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 13.07s | BCE 0.0004 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 13.16s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 13.25s | BCE 0.0004 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 13.34s | BCE 0.0004 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 13.44s | BCE 0.0004 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 13.53s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 13.61s | BCE 0.0005 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 13.72s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 13.81s | BCE 0.0003 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 13.91s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 14.00s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 14.09s | BCE 0.0008 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 14.18s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 14.27s | BCE 0.0007 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 14.36s | BCE 0.0003 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 14.45s | BCE 0.0001 |\n",
      "| epoch   1 |  2625/   63 batches | ms/batch  5.54 | BCE 0.0004 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 14.55s | BCE 0.0004 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 14.65s | BCE 0.0003 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 14.75s | BCE 0.0003 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 14.84s | BCE 0.0009 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 14.94s | BCE 0.0004 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 15.03s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 15.12s | BCE 0.0004 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 15.20s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 15.29s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 15.38s | BCE 0.0006 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 15.48s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 15.57s | BCE 0.0007 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 15.67s | BCE 0.0003 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 15.75s | BCE 0.0003 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 15.85s | BCE 0.0003 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 15.96s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 16.06s | BCE 0.0000 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 16.16s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 16.24s | BCE 0.0003 |\n",
      "| epoch   1 |  2880/   63 batches | ms/batch  5.67 | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 16.34s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 16.46s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 16.56s | BCE 0.0003 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 16.65s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 16.75s | BCE 0.0005 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 16.85s | BCE 0.0003 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 16.94s | BCE 0.0003 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 17.03s | BCE 0.0005 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 17.14s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 17.27s | BCE 0.0003 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 17.38s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 17.47s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 17.57s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 17.68s | BCE 0.0007 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 17.81s | BCE 0.0006 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 17.94s | BCE 0.0003 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 18.04s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 18.13s | BCE 0.0004 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 18.22s | BCE 0.0003 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 18.31s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 18.41s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 18.53s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 18.63s | BCE 0.0004 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 18.76s | BCE 0.0003 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 18.86s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 18.95s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 19.05s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 19.16s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 19.27s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 19.36s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 19.45s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 19.55s | BCE 0.0003 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 19.64s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 19.73s | BCE 0.0003 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 19.83s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 19.93s | BCE 0.0003 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 20.01s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 20.09s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 20.19s | BCE 0.0005 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 20.30s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 20.41s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 20.51s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 20.62s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 20.71s | BCE 0.0003 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 20.81s | BCE 0.0003 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 20.93s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 21.03s | BCE 0.0003 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 21.13s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 21.21s | BCE 0.0002 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 21.30s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 21.41s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 21.50s | BCE 0.0003 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 21.61s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 21.70s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 21.79s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 21.88s | BCE 0.0001 |\n",
      "| epoch   1 |  4095/   63 batches | ms/batch  5.36 | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 21.97s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 22.06s | BCE 0.0004 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 22.17s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 22.26s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 22.34s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 22.45s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 22.55s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 22.64s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 22.73s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 22.82s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 22.90s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 22.99s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 23.09s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 23.18s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 23.29s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 23.38s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 23.46s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 23.56s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 23.65s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 23.74s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 23.84s | BCE 0.0003 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 23.93s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 24.03s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 24.12s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 24.20s | BCE 0.0003 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 24.33s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 24.45s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 24.56s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 24.66s | BCE 0.0003 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 24.74s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 24.82s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 24.90s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 24.99s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 25.10s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 25.20s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 25.29s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 25.38s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 25.47s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 25.56s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 25.66s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 25.74s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 25.83s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 25.92s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 26.03s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 26.12s | BCE 0.0002 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 26.22s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 26.31s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 26.41s | BCE 0.0004 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 26.51s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 26.63s | BCE 0.0002 |\n",
      "| epoch   1 |  4845/   63 batches | ms/batch  5.52 | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 26.72s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 26.81s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 26.90s | BCE 0.0001 |\n",
      "| epoch   1 |  4860/   63 batches | ms/batch  5.55 | BCE 0.0003 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 26.99s | BCE 0.0003 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 27.09s | BCE 0.0002 |\n",
      "| epoch   1 |  4890/   63 batches | ms/batch  5.56 | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 27.19s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 27.28s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 27.37s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 27.46s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 27.55s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 27.64s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 27.74s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 27.84s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 27.95s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 28.06s | BCE 0.0003 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 28.15s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 28.27s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 28.37s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 28.48s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 28.57s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 28.66s | BCE 0.0002 |\n",
      "| epoch   1 |  5130/   63 batches | ms/batch  5.61 | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 28.76s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 28.86s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 28.95s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 29.06s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 29.16s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 29.25s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 29.34s | BCE 0.0001 |\n",
      "| epoch   1 |  5220/   63 batches | ms/batch  5.64 | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 29.44s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 29.54s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 29.64s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 29.74s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 29.85s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 29.94s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 30.05s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 30.14s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 30.24s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 30.33s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 30.43s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 30.54s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 30.63s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 30.73s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 30.84s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 30.93s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 31.03s | BCE 0.0002 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 31.13s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 31.25s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 31.35s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 31.45s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 31.54s | BCE 0.0003 |\n",
      "| epoch   1 |  5595/   63 batches | ms/batch  5.66 | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 31.65s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 31.74s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 31.85s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 31.95s | BCE 0.0004 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 32.03s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 32.13s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 32.22s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 32.31s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 32.41s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 32.52s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 32.60s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 32.70s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 32.80s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 32.90s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 32.98s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 33.10s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 33.19s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 33.27s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 33.36s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 33.44s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 33.52s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 33.62s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 33.72s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 33.81s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 33.90s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 34.01s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 34.14s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 34.24s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 34.33s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 34.42s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 34.51s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 34.59s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 34.68s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 34.76s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 34.84s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 34.95s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 35.03s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 35.12s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 35.20s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 35.29s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 35.37s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 35.47s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 35.56s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 35.64s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 35.73s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 35.81s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 35.91s | BCE 0.0002 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 36.00s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 36.10s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 36.20s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 36.29s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 36.38s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 36.47s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 36.56s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 36.64s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 36.73s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 36.83s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 36.91s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 37.00s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 37.11s | BCE 0.0003 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 37.20s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 37.28s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 37.39s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 37.49s | BCE 0.0001 |\n",
      "| epoch   1 |  6960/   63 batches | ms/batch  5.40 | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 37.57s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 37.67s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 37.76s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 37.84s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 37.94s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 38.03s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 38.11s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 38.19s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 38.27s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 38.36s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 38.44s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 38.52s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 38.61s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 38.71s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 38.80s | BCE 0.0001 |\n",
      "| epoch   1 |  7200/   63 batches | ms/batch  5.40 | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 38.90s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 39.01s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 39.09s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 39.17s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 39.26s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 39.37s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 39.46s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 39.56s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 39.65s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 39.73s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 39.82s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 39.92s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 40.00s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 40.10s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 40.19s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 40.27s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 40.38s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 40.48s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 40.57s | BCE 0.0001 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 40.66s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 40.75s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 40.83s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 40.92s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 41.00s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 41.08s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 41.17s | BCE 0.0000 |\n",
      "| epoch   1 |  7680/   63 batches | ms/batch  5.37 | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 41.25s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 41.34s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 41.43s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 41.52s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 41.61s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 41.71s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 41.79s | BCE 0.0001 |\n",
      "| epoch   1 |  7770/   63 batches | ms/batch  5.39 | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 41.87s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 41.96s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 42.05s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 42.14s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 42.23s | BCE 0.0001 |\n",
      "| epoch   1 |  7830/   63 batches | ms/batch  5.40 | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 42.32s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 42.43s | BCE 0.0003 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 42.54s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 42.62s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 42.72s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 42.81s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 42.91s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 43.00s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 43.10s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 43.18s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 43.26s | BCE 0.0000 |\n",
      "| epoch   1 |  8085/   63 batches | ms/batch  5.36 | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 43.35s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 43.43s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 43.52s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 43.61s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 43.72s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 43.83s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 43.92s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 44.00s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 44.08s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 44.16s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 44.24s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 44.33s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 44.42s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 44.51s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 44.61s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 44.69s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 44.77s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 44.86s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 44.94s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 45.03s | BCE 0.0002 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 45.11s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 45.20s | BCE 0.0001 |\n",
      "| epoch   1 |  8400/   63 batches | ms/batch  5.39 | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 45.28s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 45.46s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 45.56s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 45.64s | BCE 0.0001 |\n",
      "| epoch   1 |  8610/   63 batches | ms/batch  5.31 | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 45.75s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 45.86s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 45.95s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 46.05s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 46.14s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 46.26s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 46.36s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 46.46s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 46.55s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 46.65s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 46.75s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 46.86s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 46.96s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 47.04s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 47.14s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 47.25s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 47.35s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 47.45s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 47.55s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 47.64s | BCE 0.0001 |\n",
      "| epoch   1 |  8985/   63 batches | ms/batch  5.31 | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 47.74s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 47.84s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 47.92s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 48.04s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 48.15s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 48.24s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 48.34s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 48.43s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 48.53s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 48.65s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 48.73s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 48.83s | BCE 0.0001 |\n",
      "| epoch   1 |  9165/   63 batches | ms/batch  5.34 | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 48.92s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 49.01s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 49.09s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 49.19s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 49.28s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 49.40s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 49.50s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 49.60s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 49.69s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 49.78s | BCE 0.0002 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 49.88s | BCE 0.0000 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 49.99s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 50.09s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 50.19s | BCE 0.0001 |\n",
      "| epoch   1 |  9390/   63 batches | ms/batch  5.35 | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 50.28s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 50.38s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 50.48s | BCE 0.0001 |\n",
      "| epoch   1 |  9435/   63 batches | ms/batch  5.36 | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 50.58s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 50.68s | BCE 0.0000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 50.78s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 50.88s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 50.97s | BCE 0.0001 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 51.05s | BCE 0.0001 |\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-225-f141fe0f46a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_AN3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_epoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtraining_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_AN3\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mvalidation_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_AN3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtraining_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-224-ef8c50641b14>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, epoch, versatile)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;31m# automatically calculate the backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0;31m# perform the actual backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/envTF113/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/envTF113/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_epoch = 60  # train the model for 100 epochs\n",
    "model_save = 'best_model_SV3.pt'  # path to save the best validation model\n",
    "\n",
    "# main function\n",
    "\n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "\n",
    "model_AN3 = AlexNet()\n",
    "optimizer = optim.Adam(model_AN3.parameters(), lr=1e-4)\n",
    "for epoch in range(1, total_epoch + 1):\n",
    "    training_loss.append(train(model_AN3 , epoch))\n",
    "    validation_loss.append(validate(model_AN3, epoch))\n",
    "    if training_loss[-1] == np.min(training_loss):\n",
    "        print('      Best training model found.')\n",
    "    if validation_loss[-1] == np.max(validation_loss):\n",
    "        # save current best model on validation set\n",
    "        with open(model_save, 'wb') as f:\n",
    "            torch.save(model_AN3.state_dict(), f)\n",
    "            print('      Best validation model found and saved.')\n",
    "    \n",
    "    print('-' * 99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you finish the model training, evaluate your model by calculating the EER for the verification test. Compare the EER between the multiclass classification model and thie siamese network with triplet loss. What do you find? How can you improve the performance of the model with the worse performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: evaluate the model, calculate EER as above, and make the comparison\n",
    "# remember to re-generate the average embeddings and the test embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: enter your observations and comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion: Conventional Speaker Recognition Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to neural networks, the most popular methods for speaker recognition are [**Joint Factor Analysis (JFA)**](https://www.researchgate.net/profile/Patrick_Kenny/publication/228922248_Joint_factor_analysis_of_speaker_and_session_variability_Theory_and_algorithms/links/00b49519e4362ef144000000/Joint-factor-analysis-of-speaker-and-session-variability-Theory-and-algorithms.pdf) and [**i-vector**](https://ieeexplore.ieee.org/abstract/document/5545402). \n",
    "\n",
    "The general idea of these two methods is straightforward. Both methods attempt to present *all possible speech utterances* in a shared embedding space (like a Gaussian Mixture Model) - you can imagine that it is a generative model such that by assigning proper sampling strategies you can obtain speech utterances from different speakers. The embedding space contains everything including phonemes, pitch, and speaker identity, and for each utterance you have different weights applied to the embeddings of each of these characteristics, and this embedding space requires a training step with available data (like when we train the networks). The weight for the speaker identity characteristic is used as the speaker embeddings that distinguish different speakers. During model training, similar approaches can be applied like what we do here - we can still train a classifier such that speaker embeddings from the utterances from the same speaker are close to each other, and embeddings from the utterances from the different speakers are far from each other. This is typically done by Bayesian approaches.\n",
    "\n",
    "Compared with the neural network approach, the embedding space is replaced by the mapping function defined by the neural network - we assume that the neural network itself is a strong enough feature extractor that only cares about the speaker identity and *removes* all the other characteristics in a given utterance. JFA and i-vector are both linear models and their performance can be constrained due to the model capacity, while the nonlinearity of the neural networks can better model the large amount of data and have a better representation and feature extraction power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion: Scoring Method\n",
    "\n",
    "We are using the most simple way of scoring with cosine similarity score. However, the most widely-used method for scoring is [***probabilistic linear discriminant analysis (PLDA)***](https://towardsdatascience.com/probabilistic-linear-discriminant-analysis-plda-explained-253b5effb96). PLDA is a probablistic framework that estimates the probability that a given speaker embedding belongs to the target speaker's speaker embeddings (like a GMM). For the details of how to apply PLDA scoring, you can check [this paper](https://ieeexplore.ieee.org/document/7078610) or [this implementation](https://github.com/RicherMans/PLDA)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
